Narration:
Today, placing an order requires filling rigid tickets. Traders think in intent, not fields.
Our GUI allows users to express intent in natural language, like 'Buy 100k AAPL with minimal impact'.
The system converts that intent into a structured order and explains what it understood.
This is decision support, not auto-trading. The user always reviews and controls the outcome.

Narration:
Once intent is understood, the AI suggests an execution strategy.
In phase one, we intentionally limit scope to VWAP, TWAP, and POV for safety and transparency.
The AI explains why it recommends each strategy—based on order size, urgency, market liquidity, and your past preferences.
Critically, user corrections are treated as learning signals, not errors.
You can accept, modify, or completely override any suggestion.

Narration:
Learning is the most critical and most misunderstood part.
There is no real-time learning in production. Inference and execution are fully deterministic.
User corrections are captured and analyzed offline, typically end of day.
APO reinforcement learning improves prompts and strategy mapping, not execution logic.
Every improvement is reviewed and approved—typically before deployment, and can be rolled back at any time.
Inference is real-time. Learning is offline. Humans stay in control.

Narration:
This diagram shows our complete technical architecture with clear separation of concerns.
The blue section represents real-time inference—fully deterministic with no learning.
When a trader enters an order, it flows through our vendor-neutral agent layer to the LLM for strategy suggestion.
The user reviews and confirms. If they make a correction, that flows to the green section—the offline learning path.
End of day, we run APO reinforcement learning on accumulated corrections.
Every change goes through human review before deployment.
Notice the dashed line from governance back to the agent—that's our rollback capability.
